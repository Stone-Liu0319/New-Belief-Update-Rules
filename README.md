This repository contains the paper for New Rules for Updating Belief for Parameters Shared Across Data Sets. It considers several combinations of semi-modular inference and beta-divergence loss, and comparing their predictive performance in terms of elpd (expected log pointwise predictive density) on the biased normal data.
We observe similar structure between ELPD and Safe Bayes (Gr{\"u}nwald and Van Ommen, 2017), and Safe Bayes has theoretical support that it converges at the optimal rate to the pseudo-true as the data grows. However, ELPD is much easier to compute.Does the ELPD behave like Safe Bayes? Besides, we will experiment with more examples to test our methods.

      We aim to find good practical new belief update rules.
